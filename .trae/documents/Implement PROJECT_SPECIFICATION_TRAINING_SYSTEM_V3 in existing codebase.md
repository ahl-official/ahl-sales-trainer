## Overview
I have read `PROJECT_SPECIFICATION_TRAINING_SYSTEM_V3.md` carefully and compared it with the current backend (Flask), database schema, PDF generator, and `trainer.html` frontend.

The codebase already has:
- `question_bank` and `answer_evaluations` tables with objection-related fields.
- Backend question generation and evaluation helpers (`prepare_questions_internal`, `evaluate_answer_internal`) with Pinecone + OpenRouter.
- Training endpoints: `/api/training/start`, `/api/training/prepare`, `/api/training/evaluate-answer`, `/api/training/get-next-question`, `/api/training/message`, `/api/training/end`, `/api/training/report` (POST/GET), and PDF export.
- Frontend flow in `trainer.html` that still uses the **old conversational pattern** (AI as customer using RAG directly), not the new question-bank + per-answer evaluation flow.

The main gaps vs the spec are:
- Frontend still drives a generic conversation instead of using pre-generated questions and real-time scoring.
- `/api/training/message` does not trigger evaluation and does not store evaluation attached to messages.
- Reports are generated by the frontend via a generic LLM prompt; they are not built from `question_bank` + `answer_evaluations` with question-by-question breakdown and objection scores.
- Adaptive difficulty and objection-handling reporting are only partially implemented.

Below is a phase-by-phase plan to align the codebase with the spec, using the existing structures and minimizing breaking changes.

## Phase 1 – Database & Data Model Alignment
- Confirm that `question_bank` and `answer_evaluations` tables fully match spec:
  - `question_bank`: question text, expected answer, key points JSON, source, difficulty, `is_objection` (already present).
  - `answer_evaluations`: per-answer scores including objection-related fields (`objection_score`, `technique_adherence`).
- Decide how to handle spec’s extra fields without over-complicating schema:
  - Keep `question_ids` for a session implicit via `question_bank.session_id` instead of adding an array column.
  - Add an optional JSON column `evaluation_data` on `messages` to store the raw evaluation summary for each user answer (aligning with spec, but using a single JSON field instead of many flat columns).
- Ensure that `sessions` already has `overall_score` and `notes` (it does) and no schema changes are required there beyond what exists.
- Non-code step (once system is ready): prepare `Sales_Objections_Master_Script.txt` exactly as in the spec and upload it as training content under category "Sales Objections" and video name "Master Objection Handling Script" via the existing admin upload page.

## Phase 2 – Question Generation (FR-1 + Objection Integration)
- Refine `prepare_questions_internal` in `backend/app.py` to fully match FR-1:
  - Enforce question counts per difficulty:
    - New Joining: 5–7 mostly factual
    - Basic: 4–5 factual + 2–3 procedural
    - Experienced: mix factual/procedural/scenario as per spec
    - Expert: more scenario/edge-case questions
  - When category name contains "objection" or is specifically "Sales Objections":
    - Make sure the combined Pinecone content always includes the Master Objection Script (this will happen naturally once uploaded, but we can emphasize it in the system prompt).
    - Clearly instruct the LLM to tag suitable questions with `is_objection: true` and to cover the specific objection patterns listed in the spec.
  - Ensure the JSON output from the LLM always contains, for each question:
    - `question`, `expected_answer`, `key_points`, `source`, `difficulty`, `is_objection`.
- Keep question generation logic in `app.py` for now (instead of separate `question_generator.py`) to avoid a large refactor; optionally, a later refactor can extract it if needed.

## Phase 3 – Answer Evaluation & Objection Rubric (FR-2, FR-3, FR-6)
- Enhance `evaluate_answer_internal` in `backend/app.py`:
  - For **standard questions**:
    - Guarantee the JSON contains `accuracy`, `completeness`, `clarity`, `overall_score`, `what_correct`, `what_missed`, `what_wrong`, `feedback`, `evidence_from_training`.
  - For **objection questions** (`is_objection` true):
    - Extend the prompt to embed the full evaluation rubric from the spec: tone, technique, closing, forbidden mistakes, bonus for prescribed language, etc.
    - Require the model to output numeric fields: `tone`, `technique`, `closing`, `objection_score`, `overall_score`, plus booleans and lists: `prescribed_language_used`, `forbidden_mistakes_made`, and explanatory text fields.
  - Make sure we always attach `user_answer` into the evaluation dict before saving.
- Update `Database.save_answer_evaluation` to:
  - Persist `objection_score` and `technique_adherence` consistently using the evaluation JSON.
  - Avoid failing if some optional fields are missing (defensive defaults).
- Add a small helper to compute a per-session metrics summary from `answer_evaluations` (average score, factual/procedural/scenario subtotals, objection-handling score for Sales Objections) that will be reused in reports.

## Phase 4 – Backend APIs & Message/Evaluation Integration
- Keep the dedicated evaluation endpoint `/api/training/evaluate-answer` but integrate it more closely with the conversation flow:
  - Treat this as the primary way to evaluate answers; the frontend will call it after each user response.
- Extend `/api/training/message` to optionally accept `evaluation_data` and tie messages to evaluations:
  - Add an `evaluation_data` JSON column in `messages` (Phase 1).
  - After a successful call to `/api/training/evaluate-answer`, the frontend can:
    - Store the evaluation in `answer_evaluations` (already done by backend) and
    - Post a message with `role='system'` or `role='assistant'` summarizing feedback, with `evaluation_data` attached, so resuming a session shows both Q&A and feedback.
- Keep `/api/training/get-next-question` as the single source of truth for which question to ask next.
- Optionally add a helper endpoint (or reuse existing ones) to fetch all evaluations for a session when generating reports.

## Phase 5 – Enhanced Reporting (FR-4 + Objection Summary)
- Implement a server-side `generate_enhanced_report(session_id)` function (either inside `app.py` or in a new `reports.py` helper) that:
  - Loads from DB:
    - All `question_bank` items for the session in position order.
    - All `answer_evaluations` for the session.
    - Session and user metadata (`sessions`, `users`).
  - For each question, constructs a record with:
    - Question text
    - User answer (from `answer_evaluations.user_answer`)
    - Expected answer
    - Score (overall_score and, for objection questions, objection_score and technique adherence flags)
    - "What was correct", "What was missed/wrong", evidence snippets, and source reference.
  - Computes per-category subscores required by the spec:
    - Factual Knowledge, Procedural Understanding, Scenario Handling, Communication Clarity, and **Objection Handling** (for Sales Objections sessions).
  - Builds a structured HTML report (Tailwind-based) with sections:
    - Summary (overall scores and quick assessment).
    - Per-dimension scores.
    - Question-by-question breakdown.
    - Strengths, weaknesses, and recommendations with references to video/section.
    - Specific objection-handling summary if the session category is Sales Objections.
- Change report generation flow to be **backend-driven**:
  - Add an internal call in `/api/training/report/<session_id>` (GET) to generate the enhanced HTML if no report exists yet, then save it via `db.save_report`.
  - Optionally keep the POST `/api/training/report/<session_id>` path for backwards compatibility, but prefer the backend-generated variant for new flows.
- Ensure `pdf_generator.clean_html_for_reportlab` and `generate_session_pdf` work smoothly with the new HTML structure (they already clean and convert generic HTML, so minimal changes should be needed).

## Phase 6 – Frontend Trainer Flow Refactor (FR-1..4, FR-6)
- Replace the old conversational RAG loop in `frontend/trainer.html` with a **question-driven** training flow:
  - After `/api/training/start` returns, either use `prepared_questions` from the response or immediately call `/api/training/get-next-question` to get the first question.
  - Display this question as the AI/customer’s message.
  - After the user speaks and we get the final transcript:
    - Call `/api/training/evaluate-answer` with `{session_id, question_id, user_answer}`.
    - Use the returned evaluation to:
      - Show immediate feedback in the chat (as a short, friendly message summarizing score and key corrections), following FR-3’s rules for positive/constructive/corrective tone based on the numeric score.
      - Optionally display a small per-question score chip in the UI.
    - Store the feedback in the conversation via `/api/training/message` with `evaluation_data` attached.
  - Then call `/api/training/get-next-question` for the next one; if no more questions, or time expires, end the session.
- Ensure **AI stays in the customer role**:
  - Instead of directly calling `/api/ai/chat` for questions, questions are pre-generated from training content and presented as customer questions.
  - The only LLM call during answering is evaluation, not free-form question generation.
- For **Sales Objections** sessions:
  - Optionally highlight in the UI (e.g., a badge or banner) that objection-handling techniques are being evaluated.
  - Surface objection-specific scores in the report view once the GET `/api/training/report` returns enhanced HTML.
- Update the report screen to simply render the HTML returned by the backend and, if useful, show an “Export PDF” button that hits `/api/sessions/{session_id}/export/pdf`.

## Phase 7 – Adaptive Difficulty (FR-5) and UX Enhancements
- Implement adaptive question selection in `get_next_unanswered_question` logic:
  - Compute a running average score for the session from `answer_evaluations`.
  - If average ≥ 8, prefer questions whose `difficulty` is higher, and prioritize scenario questions.
  - If average < 5, prefer simpler questions and follow-ups (we can encode a simple difficulty ordering and use SQL + Python logic to pick the next question accordingly).
  - If certain key points or topics are repeatedly missed (we can infer from evaluation_data or question tags), pick additional questions touching those topics when available.
- Add small UX touches in `trainer.html`:
  - Progress indicator (e.g., “Question 3 of 8”).
  - Live score trend (optional, or only in the final report).

## Phase 8 – Testing & Validation
- Backend tests:
  - Extend `backend/tests/test_training_flow.py` to cover:
    - Question preparation by difficulty and objection category.
    - Evaluation saving logic for standard vs objection questions.
    - Enhanced report generation returning the expected HTML structure and including objection-handling scores.
  - Add focused tests for DB helpers in `test_database.py` around `question_bank` and `answer_evaluations`.
- Frontend tests / manual flows:
  - Verify the complete flow: start session → receive question → answer → get real-time feedback → session end → view report → export PDF.
  - Explicitly test a Sales Objections session to ensure the Master Script questions and objection-handling scores appear.
- Performance checks:
  - Informally confirm that question generation (<10s), answer evaluation (<3s), and report generation (<5s) are within NFR budgets, adjusting prompts or top_k for Pinecone if needed.

If you confirm this plan, I will start with Phase 1 and Phase 2 (DB alignment + question generation and objection integration), then move forward phase by phase, keeping the implementation aligned strictly with the specification document.